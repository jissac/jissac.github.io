<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.5.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="James Issac">

  
  
  
    
  
  <meta name="description" content="Over the past few decades, the digitization of our society has led to massive amounts of data being stored. Combining this increase in the scale of stored information with advances in hardware computational power and algorithmic innovations, the field of artificial intelligence (AI) has jumped into the spotlight as machines seem to possess the ‘magical’ ability to learn without being told explicitly what to do.
Examples of impressive feats performed by machines include: AlphaGo defeating 9dan rank Go champion Lee Sedol, self-driving cars navigating city streets, and a computer learning to beat Super Mario World by itself.">

  
  <link rel="alternate" hreflang="en-us" href="https://jamesis.me/post/deep-learning-lr/">

  


  
  
  
  <meta name="theme-color" content="rgb(53,219,146)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://jamesis.me/post/deep-learning-lr/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="James Issac">
  <meta property="og:url" content="https://jamesis.me/post/deep-learning-lr/">
  <meta property="og:title" content="Understanding Deep Neural Networks from First Principles: Logistic Regression | James Issac">
  <meta property="og:description" content="Over the past few decades, the digitization of our society has led to massive amounts of data being stored. Combining this increase in the scale of stored information with advances in hardware computational power and algorithmic innovations, the field of artificial intelligence (AI) has jumped into the spotlight as machines seem to possess the ‘magical’ ability to learn without being told explicitly what to do.
Examples of impressive feats performed by machines include: AlphaGo defeating 9dan rank Go champion Lee Sedol, self-driving cars navigating city streets, and a computer learning to beat Super Mario World by itself."><meta property="og:image" content="https://jamesis.me/post/deep-learning-lr/featured.jpeg">
  <meta property="twitter:image" content="https://jamesis.me/post/deep-learning-lr/featured.jpeg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2018-04-18T16:41:18-05:00">
    
    <meta property="article:modified_time" content="2018-04-18T16:41:18-05:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jamesis.me/post/deep-learning-lr/"
  },
  "headline": "Understanding Deep Neural Networks from First Principles: Logistic Regression",
  
  "image": [
    "https://jamesis.me/post/deep-learning-lr/featured.jpeg"
  ],
  
  "datePublished": "2018-04-18T16:41:18-05:00",
  "dateModified": "2018-04-18T16:41:18-05:00",
  
  "author": {
    "@type": "Person",
    "name": "James Issac"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "James Issac",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jamesis.me/img/icon-512.png"
    }
  },
  "description": "Over the past few decades, the digitization of our society has led to massive amounts of data being stored. Combining this increase in the scale of stored information with advances in hardware computational power and algorithmic innovations, the field of artificial intelligence (AI) has jumped into the spotlight as machines seem to possess the ‘magical’ ability to learn without being told explicitly what to do.\nExamples of impressive feats performed by machines include: AlphaGo defeating 9dan rank Go champion Lee Sedol, self-driving cars navigating city streets, and a computer learning to beat Super Mario World by itself."
}
</script>

  

  


  


  





  <title>Understanding Deep Neural Networks from First Principles: Logistic Regression | James Issac</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">James Issac</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Articles</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#photos"><span>Photos</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Understanding Deep Neural Networks from First Principles: Logistic Regression</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 18, 2018
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    11 min read
  </span>
  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 405px;">
  <div style="position: relative">
    <img src="/post/deep-learning-lr/featured_huf25920c35b2db2fc61db7dcf23cd8987_1998149_720x0_resize_q90_lanczos.jpeg" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Over the past few decades, the digitization of our society has led to massive amounts of data being stored. Combining this increase in the scale of stored information with advances in hardware computational power and algorithmic innovations, the field of artificial intelligence (AI) has jumped into the spotlight as machines seem to possess the ‘magical’ ability to learn without being told explicitly what to do.</p>
<p>Examples of impressive feats performed by machines include: AlphaGo defeating 9dan rank Go champion Lee Sedol, self-driving cars navigating city streets, and a computer learning to beat Super Mario World by itself. At the heart of such systems have been algorithms called deep neural networks that can model the non-linearities inherently present in image, audio, and video data.</p>
<p>So… why is this important?</p>
<p>My goal in this article is to convince you that these recent advances in AI aren’t the purview of a select few but that if you’re interested, you too can learn about and contribute to this amazing field. We’ll start by peeling back the layers of abstraction surrounding deep neural networks and begin by using logistic regression as our first principle starting point (had to start somewhere!). I’ll assume you have basic knowledge of matrix math(<a href="https://www.youtube.com/watch?v=LyGKycYT2v0">dot products</a>), calculus (<a href="https://www.youtube.com/watch?v=AXqhWeUEtQU">partial derivatives</a>), probability theory (<a href="https://www.youtube.com/watch?v=JGeTcRfKgBo">conditional probability</a>), and <a href="https://www.codecademy.com/learn/learn-python">Python programming</a>. If not, do not fear! Follow the links above and keep learning — if you have an internet connection and an unyielding thirst for knowledge, be patient with yourself and you’ll soon be slaying all the proverbial giants. I’ve linked references and resources at the end as learning aids — after all, standing on shoulders is a great way to see further ahead.</p>
<p>So, by understanding how logistic regression can be modeled as a single neuron you’ll understand fundamental deep learning concepts like <strong>weights</strong>, <strong>activation functions</strong>, <strong>loss functions</strong>, <strong>gradient descent</strong>, <strong>learning rate</strong>, <strong>training</strong>, <strong>forward and backward propagation</strong>, and <strong>prediction</strong>. You’ll then be able to move on to more advanced topics that link many neurons together into deep networks — like Convolutional Neural Nets (CNNs), Generative Adversarial Nets (GANs), and Recurrent Neural Nets(RNNs), to name a few. So let’s get started!</p>
<h2 id="a-neuron">A Neuron</h2>
<p>Let’s use the word ‘neuron’ to describe a function that looks like this:
<img src="neuron.png" alt="neuron"></p>
<p>You have inputs and you have an output (this a 1-layer neural net — by convention we don’t count the initial raw inputs as a layer). If you squint hard enough, it even sort of looks like a <a href="http://s4.thingpic.com/images/E6/BdHJAWutQ3E5zkvcgFz8Kk8x.png">human neuron</a>. This function is defined as a weighted sum of its inputs — we multiply the inputs <em><strong>x</strong></em> by variables <em><strong>w</strong></em> called weights to get the output. You can think of weights as the strengths of the connections between input and output — for example, if <em>w1</em> has a higher value than <em>w2</em>, that implies that the input x1 influences the output more than <em>x2</em> does. The value <em>b</em> is called the bias term (it isn’t multiplied by a weight) and is responsible for shifting the function so that it’s not constrained to the origin. Representing the function in matrix form yields:
<img src="neuron-matrix-form.png" alt="neuron-matrix-form"></p>
<p>The shape of the matrix <em>w</em> is determined by the number of units in the layer you’re mapping to and the number of units in the layer you’re mapping from— hence the shape [1 x 3] because the output layer has 1 unit and the input layer has 3 units (not counting the bias term). This looks a lot like the equation for a line:
<img src="line-eqn.png" alt="line-eqn"></p>
<p>However, the strength of a neural network lies in its ability to model complex nonlinearities. Even if you link many of these units together in a deep network (the green units are called hidden layers) as shown below,</p>
<p><img src="3-layer-network.png" alt="3 layer neural network"></p>
<p>the composition of many linear functions is itself a linear function. So our journey is not yet complete — how can the function model the non-linear patterns present in the input data in such a way that it can predict, with high accuracy, similar patterns in data it has never seen before?</p>
<p>To model a non-linear problem, we will need to introduce a nonlinear activation function.</p>
<h2 id="the-sigmoid-function">The Sigmoid Function</h2>
<p>Let’s define a function called sigmoid. It has a probability distribution that looks like this:</p>
<p><img src="sigmoid.png" alt="Source: https://commons.wikimedia.org/w/index.php?curid=4310325"></p>
<p>The equation for a sigmoid function is:
<img src="sigmoid-eqn.png" alt="sigmoid eqn">
The sigmoid activation function converts its input to a value between 0 and 1 — as z increases towards positive infinity the output gets closer to 1, and as z decreases towards negative infinity the output gets closer to 0. Going back to our example of a single neuron, we can feed our function <em>z = wx + b</em> as an input to the sigmoid activation function, which yields:
<img src="neuron-sigmoid-eqn.png" alt="sigmoid neuron eqn">
Where the variable A represents the sigmoid activation function. Showing it pictorially using our single-layer representation of a neuron:</p>
<p><img src="neuron-sigmoid.png" alt="Applying the sigmoid activation function">
So how is this valuable? Stacking nonlinearities on nonlinearities in a deep network allows us to model very complex relationships between inputs and outputs. Even in the case of our single neuron above, let’s see how adding a nonlinear activation function can be useful.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<p><a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic regression</a> is a binary classification method. For example, after being trained on images of cats and dogs and then being given a picture that it has never seen before of a cat (y=0) or a dog (y=1), can the machine predict the correct type? As we’ll see, even a simple algorithm like logistic regression can do surprisingly well.
We want to model a function that can take in any number of inputs and constrain the output to be between 0 and 1. Where have we seen that before? You guessed it — our humble neuron, armed with the sigmoid activation function, boldly comes in to save the day! Now is there a way to measure how good our predicted output is compared to the true label?</p>
<h3 id="loss-function-and-cost-function">Loss Function and Cost Function</h3>
<p>Let’s define <em>y</em> as the true label (y = 0 or y = 1) and <em>y_hat</em> as the predicted output (or the probability that y = 1 given inputs w and x). Therefore, the probability that y = 0 given inputs <em>w</em> and <em>x</em> is (1 - <em>y_hat</em>), as shown below.
<img src="loss-prob.png" alt="Loss probability">
<a href="http://cs229.stanford.edu/notes/cs229-notes1.pdf">After doing some derivations</a> based on the equations above, we can define the logistic loss function for a set of inputs in a single training example to be:
<img src="log-loss.png" alt="Log loss"></p>
<p>The goal of the loss function is to minimize the error between the predicted and desired output and thus arrive at an optimal solution for one training example. However, to get useful results we need to take the average of the loss function over an entire training set that contains many examples (for a total of m examples). This is defined as the cost function <em><strong>J(w,b)</strong></em> and we’ll find the parameters <em><strong>w</strong></em> and <em><strong>b</strong></em> that minimize the overall cost function:
<img src="cost-fn.png" alt="cost fn"></p>
<h3 id="gradient-descent">Gradient Descent</h3>
<p>Plotting the cost function <em><strong>J(w,b)</strong></em> yields a graph that looks like this:
<img src="gradient-descent.png" alt="gradient-descent">
One of the reasons we use this cost function for logistic regression is that it’s a convex function with a single global optimum. Imagine rolling a ball down the bowl-shaped function — it would settle at the bottom; similarly, to find the minimum of the cost function, we need to get to the lowest point. To do that, we can start from anywhere on the function and iteratively move down in the direction of the steepest slope, adjusting the values of w and b that lead us to the minimum. The formulas are:
<img src="gd-formula.png" alt="gd-formula">
In these two equations, the partial derivatives <em>dw</em> and <em>db</em> represent the effect that a change in <em>w</em> and <em>b</em> have on the cost function, respectively. By finding the slope and taking the negative of that slope, we ensure that we will always move in the direction of the minimum. To get a better understanding, let’s see this graphically for <em>dw</em>:
<img src="gd-graph.png" alt="gd-graph">
When the derivative term is positive, we move in the opposite direction towards a decreasing value of w and when the derivative is negative we move in the direction of increasing w, thereby ensuring that we’re always moving toward the minimum.</p>
<p>The alpha term in front of the partial derivative is called the learning rate and is a measure of how big a step to take at each iteration. The choice of learning parameters is an important one — too small and the model will take an undue amount of time to find the minimum, too large and the model might overshoot the minimum and fail to converge.</p>
<p>Gradient descent is the essence of the learning process — through it the machine learns what values of weights and biases minimize the cost function. It does this by iteratively comparing its predicted output for a set of data to the true output in a process called training.</p>
<h2 id="training-a-model">Training a Model</h2>
<p>An athlete is able to perform at a high level because of her extensive training — through repeated iterations and adjustments along the way, she’s able to figure out what works and what doesn’t. Similarly in supervised machine learning, given a set of inputs and output labels, a model learns the best combination of weights and biases that minimizes the overall cost function.</p>
<p>(On a high level — when people talk about machine intelligence, it’s important to distinguish between this idea of learning as opposed to the more abstract idea of artificial consciousness, which is harder to quantify. The advanced feats we’ve seen machines do thus far have basically been examples of clever optimization techniques). So what does this learning process look like?</p>
<h3 id="forward-propagation">Forward Propagation</h3>
<p>First, weight and bias values are propagated forward through the model to arrive at a predicted output. At each neuron/node, the linear combination of the inputs is then multiplied by an activation function as described above— the sigmoid function in our example. This process by which weights and biases are propagated from inputs to output is called forward propagation. After arriving at the predicted output, the loss for the training example is calculated. This left-to-right process for a single example is represented in the computation graph below (recall that the prediction <em>y_hat</em> equals <em>A</em>):
<img src="forward-prop.png" alt="forward-prop"></p>
<h3 id="backward-propagation">Backward Propagation</h3>
<p>We previously saw how the gradient descent algorithm allows us to find the minimum of the cost function. Back propagation is the process of calculating the partial derivatives from the loss function back to the inputs, thereby updating the values of w and b that lead us to the minimum. It’s helpful writing out the partial derivatives starting from <em>dA</em> to see how to arrive at <em>dw</em> and <em>db</em>. Using the chain rule of calculus yields:
<img src="chain-rule.png" alt="chain-rule">
This right-to-left process that results in updated parameters w and b is represented in the computation graph below, where the results of the derivations are shown:
<img src="back-prop.png" alt="back-prop"></p>
<h3 id="prediction">Prediction</h3>
<p>After finding the optimal values of <em>w</em> and <em>b</em> after a certain number of iterations, the next step is to use those values to calculate the final predicted output. The sigmoid activation function yields a probability distribution between 0 and 1 — for logistic regression we need to convert that to a discrete value of either 0 or 1. To do so, we’ll apply a threshold value to the output, 0.5 for instance, so that probability values 0.5 or above result in a predicted output value of 1, whereas probability values less than 0.5 result in a predicted output value of 0.</p>
<p>After predicting the final output, we need to see how well the model did. One way to evaluate classification models is by defining a term called accuracy. It’s the fraction of the predictions that our model got right:
<img src="accuracy.png" alt="accuracy">
The figure below summarizes the iterative process through which the machine learns:
<img src="model-train.png" alt="overall training"></p>
<h2 id="practical-example">Practical Example</h2>
<p>Alright, enough theory, time for application! Remember the cat vs. dog classification problem — given an image of either a cat or a dog, will the computer be able to classify it as one or the other? Let’s implement a single layer neural net representation of logistic regression from scratch using Python. We’ll use Kaggle’s dogs vs. cats dataset and build the model step by step: I’ve created a <a href="https://github.com/jissac/ScratchML">Github repository called ScratchML that contains the CatsvsDogs_Logistic_Regression Jupyter notebook</a>. We’ll go through preparing and pre-processing the data (often the most time-consuming part of the journey) and then define and run our model. Try to understand what each method does and how the formulas above are being implemented. Then re-write them yourself in order to build your intuition.
After training and running the model, our humble representation of logistic regression managed to get around 69% of the test set correctly classified — not bad for a single layer neural network! Using cutting edge architectures will yield world-class results — but they are built using much of the same principles we learned implementing logistic regression.</p>
<h2 id="now-what">Now what?</h2>
<p>As is the nature of learning, we’ve only scratched the surface. We’ve covered a general overview of the major concepts, but there are yet many more topics to learn such as implementing regularization to prevent overfitting of the data, other activation functions (ReLU, tanh, etc.), stochastic gradient descent (SGD), k-fold cross validation, to name a few.</p>
<p>I’ve put together a <a href="https://github.com/jissac/Machine-Learning-Resources">list of machine learning resources</a> that should get you started on your journey. Keep learning and implementing what you learn, after all, it’s an iterative process! Happy descent — may you reach the optimum point! :)</p>
<h2 id="resources">Resources:</h2>
<ul>
<li><a href="http://www.deeplearningbook.org/contents/optimization.html">http://www.deeplearningbook.org/contents/optimization.html</a></li>
<li><a href="https://www.coursera.org/learn/neural-networks-deep-learning">https://www.coursera.org/learn/neural-networks-deep-learning</a></li>
<li><a href="http://cs231n.github.io/neural-networks-1/">http://cs231n.github.io/neural-networks-1/</a></li>
<li><a href="https://stats.stackexchange.com/questions/213325/neural-network-meaning-of-weights">https://stats.stackexchange.com/questions/213325/neural-network-meaning-of-weights</a></li>
<li><a href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks">https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks</a></li>
</ul>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/data-science/">data-science</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://jamesis.me/post/deep-learning-lr/&amp;text=Understanding%20Deep%20Neural%20Networks%20from%20First%20Principles:%20Logistic%20Regression" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://jamesis.me/post/deep-learning-lr/&amp;t=Understanding%20Deep%20Neural%20Networks%20from%20First%20Principles:%20Logistic%20Regression" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Understanding%20Deep%20Neural%20Networks%20from%20First%20Principles:%20Logistic%20Regression&amp;body=https://jamesis.me/post/deep-learning-lr/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://jamesis.me/post/deep-learning-lr/&amp;title=Understanding%20Deep%20Neural%20Networks%20from%20First%20Principles:%20Logistic%20Regression" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Understanding%20Deep%20Neural%20Networks%20from%20First%20Principles:%20Logistic%20Regression%20https://jamesis.me/post/deep-learning-lr/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://jamesis.me/post/deep-learning-lr/&amp;title=Understanding%20Deep%20Neural%20Networks%20from%20First%20Principles:%20Logistic%20Regression" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu072b14d0cbc1ae53099383f734f45939_7669307_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://jamesis.me/">James Issac</a></h5>
      
      <p class="card-text">A whole new world, that&rsquo;s where we&rsquo;ll be.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/jissac1/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/jissac" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.medium.com/@melodious" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/cloudburst_studios" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/james_issac_" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  



  </div>
</article>

      

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

      
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.d6bd04fdad2ad213aa8111c5a3b72fc5.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    2020 James Issac &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
